## 介绍
- 学习使用int 8，float16，bfloat16（brain flot16） 来压缩模型
- 学习压缩算法，存储一个32位数
## Class 1：处理大型模型
随着模型平均大小的增加，平均参数数量已达到了700亿的数量级，这是一个非常庞大的数据。然而像t4这样的低端计算卡，也只有16g的 RAM（运行内存）。所以，运行这些先进的模型仍然是一种挑战。这个时候，就要求在不需要访问高内存的情况下有效的运行这些模型。所以，社区现在面临的主要挑战是让这些模型可以通过压缩模型来访问。

而目前，最先进的模型压缩方法（如pruning and knowledge distillation)花费更多的时间去做量化。 
	1.pruning（修剪）：pruning只是包括删除模型中的层级，对模型的决策没有很重大的影响。他只是删除一些在计算过程中权重不大的某些指标的层级。
	2.knowledge distillation（知识蒸馏）：使用原模型（instructor）训练一个更小的模型（student）。知识蒸馏除了会难以避免地丢失一些信息，最主要的难题是需要确保你有足够的计算来拟合原始模型并且从中获取预测，以便可以再计算损失的时候将他们发送到原模型。但是如果你要提炼非常大的模型，那么他的成本将会指数级上涨😱。
### 量化 
量化是以较低精度表示模型权重，例如下图的小矩阵，里面存储着一个小模型的一些参数。由于矩阵以float32格式存储（这是大多数模型的默认存储数据类型）它为每个参数分配四个字节，8bit的四倍精读，因此如下矩阵占用 36 bytes。
![alt text](/image/QQ_1721389158056.png)
如果以八位精读量化权重矩阵，那么每个参数将只得到 1 byte。因此，我们只需要9 bytes就可以存储整个权重矩阵。
![alt text](/image/QQ_1721389582129.png)
然而，这是有代价的，那就是量化误差（quantization error）。最先进的量化方法背后的主要挑战是降低这种误差，尽可能避免任何性能下降。

## Class 2：数据类型和大小
### 1.unsigned integer:
用于表示正整数，n位无符号整数的范围是0到2ⁿ-1，例如八位无符号整数的最小值为0，最大值为255。而计算机分配一个八个比特的空间来存储这个八位整数，如下图所示
![alt text](/image/QQ_1721399716867.png)
对于它的表示方法这里就不多做赘述了。这里还要讲讲有符号整数，有符号整数用来表示负整数或正整数，他有多种表现形式，但由于2‘s补码（2’s Complement Code）的常见性，我们今天研究的只有2‘s补码。它的范围是-2^n-1^次幂到2^n-1^-1。因此，对于8位有符号整数，最小值为-128，最大值为127。与无符号整数的区别在于最后一个位置的数字。正如你在下图看到的
![alt text](/image/QQ_1721403751862.png)
这个值是负数。因此，如果我们处理与前面相同的序列，我们需要在数字前添加一个负号。结果为-（128+1）=-129。

### 2.integer-pytorch
在pytorch中创建具有整数数据类型的数据并非难事，只需要正确设置``torch.dtype``。如下表所示，要创建一个8位有符号整数，只需要设置``torch.int8``作为``torrch.dtype``
![alt text](/image/QQ_1721404514237.png)
为此，我们将使用``torch.iinfo``，代码如下所示。
```python
#torch.uint8
torch.iinfo(torch.uint8)
iinfo(min=0, max=255, dtype=uint8)
```
以下是jupyter notebook代码，用来本地运行并测试代码
```python
!pip install pytorch==2.1.1

##使用torch库
import torch

##8位无符号整数的信息
torch.iinfo(torch.uint8)
iinfo(min=0, max=255, dtype=uint8)

##8位有符号整数的信息
torch.iinfo(torch.int8)
iinfo(min=-128, max=127, dtype=int8)

##下面就该轮到你们自己完成了！

##64位有符号整数的信息


##32位有符号整数的信息


##16位有符号整数的信息


```
### 3.floating
讨论完了整数，接下来赶到战场的是浮点数。浮点数由三个部分组成：
- 符号：正/负（始终为1位）
- 指数（范围）：影响数值的可表示范围
- 小数部分（精度）：影响数值的精度

FP32、BF16、FP16、FP8 是具有特定数量的指数和小数部分位数的浮点格式。
例如FP32
```
FP32
符号：1位
指数（范围）：8位
小数部分（精度）：23位
总计：32位
```
下图展示了FP32可以存储多大和多小的数字
![alt text](/image/QQ_1721407068321.png)
这类数据类型在机器学习中非常重要，因为大多数模型将他的权重存储在FP32中